# Introduction

## Background of Large Language Models (LLMs)
Large Language Models (LLMs) represent a significant leap in computational systems capable of understanding and generating human language.
Building on traditional language models (LMs) like N-gram models, LLMs address limitations such as rare word handling, overfitting, and capturing complex linguistic patterns.
Notable examples such as GPT-4 and GPT-5, leverage the self-attention mechanism within Transformer architectures to efficiently manage sequential data and understand long-range dependencies.
Key advancement include in-context learning for generating coherent text from prompts and Reinforcement Learning from Human Feedback (RLHF) for refining models using human reponses.
Techniques like prompt engineering, question-answering, and conversational interactions have significantly advanced the field of natural language processing (NLP).

## Evolution from Traditional NLP Models to State-of-the-art LLMs
Understanding LLMs requires tracing the development of language models through stages such as Statistical Language Models (SLMs), Neural Language Models (NLMs), Pre-trained Language Models (PLMs), and LLMs.

### Statistical Language Models (SLMs)
Emerging in the 1990s, SLMs analyse natural language using probabilistic methods to determine the likelihood of sentences within texts.

### Neural Language Models (NLMs)
NLMs leverage neural networks to predict word sequences, overcoming SLM limitations. Word vectors enable computers to understand word meanings.
Tools like Word2Vec represent words in a vector space where semantic relationships are reflected in vector angles.
NLMs consist of interconnected neurons organised into layers, resembling the human brain's structure.
The input layer concatenates word vectors, the hidden layer applies a non-linear activation function, and the output layer predicts subsequent words using the softmax function to transform values into a probability distribution.


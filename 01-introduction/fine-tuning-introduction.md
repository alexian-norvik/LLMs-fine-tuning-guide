# Introduction

## Background of Large Language Models (LLMs)
Large Language Models (LLMs) represent a significant leap in computational systems capable of understanding and generating human language.
Building on traditional language models (LMs) like N-gram models, LLMs address limitations such as rare word handling, overfitting, and capturing complex linguistic patterns.
Notable examples such as GPT-4 and GPT-5, leverage the self-attention mechanism within Transformer architectures to efficiently manage sequential data and understand long-range dependencies.
Key advancement include in-context learning for generating coherent text from prompts and Reinforcement Learning from Human Feedback (RLHF) for refining models using human reponses.
Techniques like prompt engineering, question-answering, and conversational interactions have significantly advanced the field of natural language processing (NLP).

## Evolution from Traditional NLP Models to State-of-the-art LLMs
Understanding LLMs requires tracing the development of language models through stages such as Statistical Language Models (SLMs), Neural Language Models (NLMs), Pre-trained Language Models (PLMs), and LLMs.

### Statistical Language Models (SLMs)
Emerging in the 1990s, SLMs analyse natural language using probabilistic methods to determine the likelihood of sentences within texts.
